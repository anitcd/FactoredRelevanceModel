/**
 * TODO: queryField[] is not used. 
 * It works on only the title of the query.
 */
package searcher;

import RelevanceFeedback.NewScore;
import static common.CommonVariables.FIELD_FULL_BOW;
import static common.CommonVariables.FIELD_ID;
import common.DocumentVector;
import common.TRECQuery;
import common.TRECQueryParser;
import java.io.BufferedReader;
import java.io.File;
import java.io.FileNotFoundException;
import java.io.FileReader;
import java.io.FileWriter;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.Collections;
import java.util.Comparator;
import java.util.List;
import java.util.Properties;
import java.util.logging.Level;
import java.util.logging.Logger;
import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.core.KeywordAnalyzer;
import org.apache.lucene.analysis.en.EnglishAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.index.DirectoryReader;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.Term;
import org.apache.lucene.index.Terms;
import org.apache.lucene.index.TermsEnum;
import org.apache.lucene.queryparser.classic.ParseException;
import org.apache.lucene.queryparser.classic.QueryParser;
import org.apache.lucene.search.BlendedTermQuery.Builder;
import org.apache.lucene.search.BooleanClause;
import org.apache.lucene.search.BooleanQuery;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.PhraseQuery;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.ScoreDoc;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.TopDocs;
import org.apache.lucene.search.TopScoreDocCollector;
import org.apache.lucene.search.similarities.AfterEffect;
import org.apache.lucene.search.similarities.AfterEffectB;
import org.apache.lucene.search.similarities.AfterEffectL;
import org.apache.lucene.search.similarities.BM25Similarity;
import org.apache.lucene.search.similarities.BasicModel;
import org.apache.lucene.search.similarities.BasicModelBE;
import org.apache.lucene.search.similarities.BasicModelD;
import org.apache.lucene.search.similarities.BasicModelG;
import org.apache.lucene.search.similarities.BasicModelIF;
import org.apache.lucene.search.similarities.BasicModelIn;
import org.apache.lucene.search.similarities.BasicModelIne;
import org.apache.lucene.search.similarities.BasicModelP;
import org.apache.lucene.search.similarities.DFRSimilarity;
import org.apache.lucene.search.similarities.DefaultSimilarity;
import org.apache.lucene.search.similarities.LMDirichletSimilarity;
import org.apache.lucene.search.similarities.LMJelinekMercerSimilarity;
import org.apache.lucene.search.similarities.MultiSimilarity;
import org.apache.lucene.search.similarities.Normalization;
import org.apache.lucene.search.similarities.Normalization.NoNormalization;
import org.apache.lucene.search.similarities.NormalizationH1;
import org.apache.lucene.search.similarities.NormalizationH2;
import org.apache.lucene.search.similarities.NormalizationH3;
import org.apache.lucene.search.similarities.NormalizationZ;
import org.apache.lucene.search.similarities.Similarity;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.FSDirectory;
import org.apache.lucene.util.BytesRef;

/**
 *
 * @author dwaipayan. Extented by anirban.
 */
public class WebDocSearcher_TRECCS {

    String          propPath;
    Properties      prop;
    IndexReader     indexReader;
    IndexSearcher   indexSearcher;
    String          indexPath;
    File            indexFile;
    String          stopFilePath;
    String          queryPath;
    File            queryFile;      // the query file
    int             queryFieldFlag; // 1. title; 2. +desc, 3. +narr
    String          []queryFields;  // to contain the fields of the query to be used for search
    Analyzer        analyzer;
    Analyzer    webDocAnalyzer;           // webDocAnalyzer

    String          runName;
    int             numHits;
    boolean         boolIndexExists;
    String          resPath;        // path of the res file
    FileWriter      resFileWriter;  // the res file writer
    List<TRECQuery> queries;
    TRECQueryParser trecQueryparser;
    String          fieldToSearch;
    int             simFuncChoice;
    float           param1, param2, param3, param4;
    
    String          TRECCSTagsClustersFilePath; // Path of the (manual) clusters (of TREC CS tags) file
    String          TRECCSTagsClustersWeightFilePath;
    String          W2VFilePath; // Path of the Word2vec trained .vec file
    List<String []> TagsClusters; // To store TREC CS tags' clusters
    List<String []> TagsClustersWeight;
    List<String []> W2Vmodel; // To store all the terms and their (200D) vectors, generated by Word2vec
    //List<TRECQuery> subQueries; // To store sub-queries (broken as per TREC CS tags clusters) for each query
    

    public WebDocSearcher_TRECCS(String propPath) throws IOException, Exception {

        this.propPath = propPath;
        prop = new Properties();
        try {
            prop.load(new FileReader(propPath));
        } catch (IOException ex) {
            System.err.println("Error: Properties file missing in "+propPath);
            System.exit(1);
        }
        //----- Properties file loaded

        // +++++ setting the analyzer with English Analyzer with Smart stopword list
        stopFilePath = prop.getProperty("stopFilePath");
        System.out.println("stopFilePath set to: " + stopFilePath);
        common.EnglishAnalyzerWithSmartStopword engAnalyzer = new common.EnglishAnalyzerWithSmartStopword(stopFilePath);
        analyzer = engAnalyzer.setAndGetEnglishAnalyzerWithSmartStopword();
        // ----- analyzer set: analyzer
        webDocAnalyzer = new common.WebDocAnalyzer();

        //+++++ index path setting 
        indexPath = prop.getProperty("indexPath");
        System.out.println("indexPath set to: " + indexPath);
        indexFile = new File(indexPath);
        Directory indexDir = FSDirectory.open(indexFile.toPath());

        if (!DirectoryReader.indexExists(indexDir)) {
            System.err.println("Index doesn't exists in "+indexPath);
            boolIndexExists = false;
            System.exit(1);
        }
        //----- index path set

        /* setting query path */
        queryPath = prop.getProperty("queryPath");
        System.out.println("queryPath set to: " + queryPath);
        queryFile = new File(queryPath);
        queryFieldFlag = Integer.parseInt(prop.getProperty("queryFieldFlag"));
        queryFields = new String[queryFieldFlag-1];
        /* query path set */
        // TODO: queryFields unused

        /* constructing the query */
        fieldToSearch = prop.getProperty("fieldToSearch", FIELD_FULL_BOW);
        System.out.println("Searching field for retrieval: " + fieldToSearch);
        trecQueryparser = new TRECQueryParser(queryPath, analyzer, fieldToSearch);
        queries = constructQueries();
        /* constructed the query */

        simFuncChoice = Integer.parseInt(prop.getProperty("similarityFunction"));
        if (null != prop.getProperty("param1"))
            param1 = Float.parseFloat(prop.getProperty("param1"));
        if (null != prop.getProperty("param2"))
            param2 = Float.parseFloat(prop.getProperty("param2"));
        if (null != prop.getProperty("param3"))
            param3 = Float.parseFloat(prop.getProperty("param3"));
        if (null != prop.getProperty("param4"))
            param4 = Float.parseFloat(prop.getProperty("param4"));

        /* setting indexReader and indexSearcher */
        indexReader = DirectoryReader.open(FSDirectory.open(indexFile.toPath()));

        indexSearcher = new IndexSearcher(indexReader);
        setSimilarityFunction(simFuncChoice, param1, param2, param3, param4);

        setRunName_ResFileName();

        File fl = new File(resPath);
        //if file exists, delete it
        if(fl.exists())
            System.out.println(fl.delete());

        resFileWriter = new FileWriter(resPath, true);

        /* res path set */
        numHits = Integer.parseInt(prop.getProperty("numHits", "1000"));  

        // Ani: Setting the file paths for Word2vec model and TREC CS tags' cluster
        TRECCSTagsClustersFilePath = prop.getProperty("TRECCSTagsClustersFilePath");
        TRECCSTagsClustersWeightFilePath = prop.getProperty("TRECCSTagsClustersWeightFilePath");
        W2VFilePath = prop.getProperty("W2VFilePath");
        System.out.println("W2VFilePath set to: " + W2VFilePath + "\nTRECCSTagsClustersFilePath set to: " + TRECCSTagsClustersFilePath + "\nTRECCSTagsClustersWeightFilePath set to: " + TRECCSTagsClustersWeightFilePath);
    }

    private void setSimilarityFunction(int choice, float param1, float param2, float param3, float param4) {

        switch(choice) {
            case 0:
                indexSearcher.setSimilarity(new DefaultSimilarity());
                System.out.println("Similarity function set to DefaultSimilarity");
                break;
            case 1:
                indexSearcher.setSimilarity(new BM25Similarity(param1, param2));
                System.out.println("Similarity function set to BM25Similarity"
                    + " with parameters: " + param1 + " " + param2);
                break;
            case 2:
                indexSearcher.setSimilarity(new LMJelinekMercerSimilarity(param1));
                System.out.println("Similarity function set to LMJelinekMercerSimilarity"
                    + " with parameter: " + param1);
                break;
            case 3:
                indexSearcher.setSimilarity(new LMDirichletSimilarity(param1));
                System.out.println("Similarity function set to LMDirichletSimilarity"
                    + " with parameter: " + param1);
                break;
            case 4:
//                indexSearcher.setSimilarity(new DFRSimilarity(new BasicModelIF(), new AfterEffectB(), new NormalizationH2()));
                BasicModel bm;
                AfterEffect ae;
                Normalization nor;
                switch((int)param1){
                    case 1:
                        bm = new BasicModelBE();
                        break;
                    case 2:
                        bm = new BasicModelD();
                        break;
                    case 3:
                        bm = new BasicModelG();
                        break;
                    case 4:
                        bm = new BasicModelIF();
                        break;
                    case 5:
                        bm = new BasicModelIn();
                        break;
                    case 6:
                        bm = new BasicModelIne();
                        break;
                    case 7:
                        bm = new BasicModelP();
                        break;
                    default:
                        bm = new BasicModelIF();
                        break;
                }
                switch ((int)param2){
                    case 1:
                        ae = new AfterEffectB();
                        break;
                    case 2:
                        ae = new AfterEffectL();
                        break;
                    default:
                        ae = new AfterEffectB();
                        break;
                }
                switch ((int)param3) {
                    case 1:
                        nor = new NormalizationH1();
                        break;
                    case 2:
                        nor = new NormalizationH2();
                        break;
                    case 3:
                        nor = new NormalizationH3();
                        break;
                    case 4:
                        nor = new NormalizationZ();
                        break;
                    case 5:
                        nor = new NoNormalization();
                        break;
                    default:
                        nor = new NormalizationH2();
                        break;
                }
//                bm = new BasicModelIF();
                indexSearcher.setSimilarity(new DFRSimilarity(bm, ae, nor));
                System.out.println("Similarity function set to DFRSimilarity with default parameters");
                break;
            case 5:
                Similarity[] sims = {
                        new BM25Similarity(param1, param2),
                        new LMJelinekMercerSimilarity(param3),
                        new LMDirichletSimilarity(param4),
                        //new DFRSimilarity(new BasicModelBE(), new AfterEffectB(), new NormalizationH1()),
                    };
                Similarity sim = new MultiSimilarity(sims);

                indexSearcher.setSimilarity(sim);
                System.out.println("Similarity function set to CombSUM(BM25, LM-JM, LM-Di)"
                    + " with parameters: k1=" + param1 + ", b=" + param2 + " (BM25); lamda=" + param3 + " (LM-JM); mu=" + param4 + " (LM-Di)");
                break;
        }
    }

    private void setRunName_ResFileName() {

        runName = queryFile.getName()+"-"+fieldToSearch+"-"+indexSearcher.getSimilarity(true).
            toString().replace(" ", "-").replace("(", "").replace(")", "");
        if(null == prop.getProperty("resPath"))
            resPath = "/home/dwaipayan/";
        else
            resPath = prop.getProperty("resPath");
        if(!resPath.endsWith("/"))
            resPath = resPath+"/";
        resPath = resPath+runName;
        System.out.println("Result will be stored in: "+resPath);
    }

    private List<TRECQuery> constructQueries() throws Exception {

        trecQueryparser.queryFileParse();
        return trecQueryparser.queries;
    }

//    class SearchResult {
//        ScoreDoc docScore;
//        Document doc;
//        SearchResult(ScoreDoc docScore, Document doc){
//            this.docScore = docScore;
//            this.doc = doc;
//        }
//    }
//    
//    public class ReRankAni implements Comparator<SearchResult> {
//        @Override
//        public int compare (SearchResult a, SearchResult b) {
//            return a.docScore.score>b.docScore.score?1:a.docScore.score==b.docScore.score?0:-1;
//        }
//    }
    public class cmpScoreDoc implements Comparator<ScoreDoc> {
        @Override
        public int compare (ScoreDoc a, ScoreDoc b) {
            //return a.score>b.score?1:a.score==b.score?0:-1;   // standard sort (ascending order)
            return a.score<b.score?1:a.score==b.score?0:-1; // reverse order
        }
    }

    public class cmpW2VModel implements Comparator<String []> {
        @Override
        public int compare (String a[], String b[]) {
            return a[0].compareTo(b[0])>0?1:a[0].compareTo(b[0])==0?0:-1;   // standard sort (ascending order)
            //return a.score<b.score?1:a.score==b.score?0:-1; // reverse order
        }
    }

    public class cmpW2VCosineSim implements Comparator<Word2vec> {
        @Override
        public int compare (Word2vec a, Word2vec b) {
            //return a.consineScore>b.consineScore?1:a.consineScore==b.consineScore?0:-1;   // standard sort (ascending order)
            return a.consineScore<b.consineScore?1:a.consineScore==b.consineScore?0:-1; // reverse order
        }
    }

    public class cmpW2VKDESim implements Comparator<Word2vec> {
        @Override
        public int compare (Word2vec a, Word2vec b) {
            //return a.consineScore>b.consineScore?1:a.consineScore==b.consineScore?0:-1;   // standard sort (ascending order)
            return a.KDEScore<b.KDEScore?1:a.KDEScore==b.KDEScore?0:-1; // reverse order
        }
    }

    public class cmpMultiRankList implements Comparator<MultipleRanklists> {
        @Override
        public int compare (MultipleRanklists a, MultipleRanklists b) {
            //return a.score>b.score?1:a.score==b.score?0:-1;   // standard sort (ascending order)
            return a.weight<b.weight?1:a.weight==b.weight?0:-1; // reverse order
        }
    }

    public class cmpMergedRankList implements Comparator<MergedRanklists> {
        @Override
        public int compare (MergedRanklists a, MergedRanklists b) {
            return a.docId.compareTo(b.docId)>0?1:a.docId.compareTo(b.docId)==0?0:-1;   // standard sort (ascending order)
            //return a.docId<b.docId?1:a.docId==b.docId?0:-1; // reverse order
        }
    }

    public int docExist(List<ScoreDoc> hits, int count, ScoreDoc doc) throws Exception {
        for (int i = 0; i < hits.size(); ++i) {
//            System.out.println("Doc No: " + indexSearcher.doc(doc.doc).get("docid") + "\t" + indexReader.document(doc.doc).get("docid"));
//            System.exit(1);
            //if(indexSearcher.doc(hits[i].doc).get("docid").equals(indexSearcher.doc(doc.doc).get("docid"))) {
            if(indexSearcher.doc(hits.get(i).doc).get("docid").equals(indexSearcher.doc(doc.doc).get("docid"))) {
                return 1;
            }
        }
        return 0;
    }

    //public ScoreDoc[] mergeRanklists(List<ScoreDoc []> hitsList, float[] hitsListWeight) throws Exception {
    public ScoreDoc[] mergeRanklists(List<MultipleRanklists> multiRankList) throws Exception {
        //ScoreDoc[] hits = new ScoreDoc[numHits];
        List<ScoreDoc> hits = new ArrayList<>();
        //List<MergedRanklists> mergedLists = updateAvgScoreRanklists(multiRankList);
        List<String> nId = new ArrayList<>();
        for (int i = 0; i < multiRankList.size(); ++i) {
            if(multiRankList.get(i).hits.length >= multiRankList.get(i).nDocs) {
                for (int j = 0; j < multiRankList.get(i).nDocs; ++j) {
                    nId.add(indexSearcher.doc(multiRankList.get(i).hits[j].doc).get("docid"));
                }
            }
        }
        List<String> nIdUniq = getUniqTerms(nId.toArray(new String[0]));
        int possibleToRetrieve = nIdUniq.size();
        System.out.println("\n||||||||||||||||||||||||||| possibleToRetrieve: " + possibleToRetrieve);
//        for (int i = 0; i < multiRankList.size(); ++i) {
//            System.out.print("List: " + i + " (Cl: " + multiRankList.get(i).tagClass + ", " + multiRankList.get(i).nDocs + " of " + multiRankList.get(i).hits.length + ")\t");
//        }
//        System.out.println();
        
        int count = 0, j = 0, k = 0, flag = 0;
        while (count < Math.min(numHits, possibleToRetrieve)) {
            for (int i = 0; i < multiRankList.size(); ++i) {
                //if((k < multiRankList.get(i).nDocs) && (count < numHits) && (!"-1".equals(multiRankList.get(i).tagClass))) {
                if((count < numHits) && (multiRankList.get(i).hits != null) && (k < multiRankList.get(i).nDocs) && (k < multiRankList.get(i).hits.length)) {
                    if(count == 0) {
                        hits.add(multiRankList.get(i).hits[k]);
                        //hits[j] = multiRankList.get(i).hits[k];
                        //System.out.println(j + ": List " + i + " " + k + "-th doc of " + multiRankList.get(i).nDocs + "\tCount: " + count);
                        j++; count++;                        
                    }
                    else if(docExist(hits, count, multiRankList.get(i).hits[k]) == 0) {
                        hits.add(multiRankList.get(i).hits[k]);
                        //hits[j] = multiRankList.get(i).hits[k];
                        //System.out.println(j + ": List " + i + " " + k + "-th doc of " + multiRankList.get(i).nDocs + "\tCount: " + count);
                        j++; count++;                        
                    }
                }
//                else if(multiRankList.get(i).nDocs > multiRankList.get(i).hits.length) {
//                    flag = 1; break;
//                }
            }
//            if(flag == 1) {
//                break;
//            }
            k++;
        }

        //Arrays.sort(hits, new cmpScoreDoc());
        Collections.sort(hits, new cmpScoreDoc());
        
//        System.exit(1);

//        for (int i = 0; i < TagsClustersWeight.size(); ++i) {
//            for (int j = 0; j < TagsClustersWeight.get(i).length; ++j) {
//                System.out.print(TagsClustersWeight.get(i)[j] + " ");
//            }
//            System.out.println();
//        }
//        System.exit(1);
        //return multiRankList.get(0).hits;
        if(hits != null) {
            return hits.toArray(new ScoreDoc[0]);
        }
        return null;
    }
    
    public List<MultipleRanklists> updateAvgScoreRanklists(List<MultipleRanklists> multiRankList) throws Exception {  // Take avg score if a doc is present in multiple ranklists.
        
        List<MergedRanklists> mergedLists = new ArrayList<>();
        List<MergedRanklists> mergedListsAvg = new ArrayList<>();
        
        if(multiRankList.size() > 0) {
            for (int i = 0; i < multiRankList.size(); ++i) {
                for (int j = 0; j < multiRankList.get(i).hits.length; ++j) {
                    MergedRanklists tempLists = new MergedRanklists();
                    tempLists.docId = indexSearcher.doc(multiRankList.get(i).hits[j].doc).get("docid");
                    tempLists.score = multiRankList.get(i).hits[j].score;
                    mergedLists.add(tempLists);
                }
            }
            Collections.sort(mergedLists, new cmpMergedRankList());
            
            String docId = mergedLists.get(0).docId;
            float sum = mergedLists.get(0).score;
            float max = mergedLists.get(0).score;
            int count = 1;
            for (int i = 1; i < mergedLists.size(); ++i) {
                if(docId.equals(mergedLists.get(i).docId)) {
                    if(mergedLists.get(i).score > max) {
                        max = mergedLists.get(i).score;
                    }
                    sum += mergedLists.get(i).score;
                    count++;
                }
                else {
                    MergedRanklists tempLists = new MergedRanklists();
                    tempLists.docId = docId;
                    //tempLists.score = sum / count;  // Avg score from all ranklists
                    tempLists.score = sum;  // Sum of the scores from all ranklists. It is eventually boosting up a doc if the doc appears in multiple ranklists
                    //tempLists.score = max + ((1.0f - max) * count / multiRankList.size());    // Weighting as per #times the doc appeared on different ranklits. Higher the #times a doc appear in different ranklists, higher the score
                    //tempLists.score = (sum / count) + ((1.0f - (sum / count)) * count / multiRankList.size());
                    mergedListsAvg.add(tempLists);
                    docId = mergedLists.get(i).docId;
                    sum = mergedLists.get(i).score;
                    max = mergedLists.get(i).score;
                    count = 1;
                }
            }
            MergedRanklists tempLists = new MergedRanklists();
            tempLists.docId = docId;
            //tempLists.score = sum / count;
            tempLists.score = sum;
            mergedListsAvg.add(tempLists);

//            for (int i = 0; i < multiRankList.size(); ++i) {
//                for (int j = 0; j < multiRankList.get(i).hits.length; ++j) {
//                    System.out.println(indexSearcher.doc(multiRankList.get(i).hits[j].doc).get("docid") + "\t" + multiRankList.get(i).hits[j].score);
//                }
//                System.out.println("----------------------------");
//            }
//            System.out.println("---------------------------------------------------------------");
//            for (int i = 0; i < mergedListsAvg.size(); ++i) {
//                System.out.println(mergedListsAvg.get(i).docId + "\t" + mergedListsAvg.get(i).score);
//            }
            for (int i = 0; i < multiRankList.size(); ++i) {
                for (int j = 0; j < multiRankList.get(i).hits.length; ++j) {
                    tempLists = new MergedRanklists();
                    tempLists.docId = indexSearcher.doc(multiRankList.get(i).hits[j].doc).get("docid");
                    int index = Collections.binarySearch(mergedListsAvg, tempLists, new cmpMergedRankList());
                    if(index >= 0) {
                        multiRankList.get(i).hits[j].score = mergedListsAvg.get(index).score;
                    }
                }
            }
        }

        return multiRankList;
    }
    
    public ScoreDoc[] retrieveMultiQuery1(TRECQuery query) throws Exception {
        ScoreDoc[] hits = null;
        List<TRECQuery> subQueries = generateSubQueries(query); 
        for (int i = 0; i < subQueries.size(); ++i) {
            if(!"-1".equals(subQueries.get(i).qClass)) {
                return retrieve(subQueries.get(i));
            }
        }
        return hits;
    }
    
    public ScoreDoc[] retrieveMultiQuery(TRECQuery query) throws Exception {
//        List<ScoreDoc []> hitsList = new ArrayList<>();
//        float[] hitsListWeight = new float[TagsClusters.size()]; // Actual (used) size of hitsListWeight would be equal to the size of hitsList
        List<MultipleRanklists> multiRankList = new ArrayList<>();
        
        int j = 0, nDoc = 0;
        List<TRECQuery> subQueries = generateSubQueries(query);   // Generating multiple sub-queries
        //System.out.println("PP: " + trecQueryparser.getAnalyzedQuery(subQueries.get(0), 1).toString(fieldToSearch).replace("(", "").replace(")", ""));
        //getExpandedQuery(subQueries.get(0));
        //System.out.println(getExpandedQuery(subQueries.get(0)).qtitle);
        //System.exit(1);
        for (int i = 0; i < subQueries.size(); ++i) {
            if(!"-1".equals(subQueries.get(i).qClass)) {
//                hitsList.add(retrieve(subQueries.get(i)));
//                hitsListWeight[j] = subQueries.get(i).qClassWeight;
                if(retrieveCustomized(subQueries.get(i)) != null) {
                    multiRankList.add(new MultipleRanklists());
                    //multiRankList.get(j).hits = retrieve(subQueries.get(i));    // Retrieval with subquery terms OLD
                    //multiRankList.get(j).hits = retrieveCustomized(subQueries.get(i));  // Retrieval with subquery terms Corrected (discarding docs with no topic contribution)
                    //multiRankList.get(j).hits = retrieve(getExpandedQuery(subQueries.get(i)));    // Retrieval with expanded (top k W2V) subquery terms
                    //multiRankList.get(j).hits = retrieveCustomized(getExpandedQuery(subQueries.get(i), 5));    // Retrieval with expanded (top k W2V) subquery terms
                    multiRankList.get(j).hits = retrieveCustomized(getExpandedQueryW2VKDE(subQueries.get(i), 5, 100));    // Retrieval with expanded (top k W2V) subquery terms
                    
                    multiRankList.get(j).weight = subQueries.get(i).qClassWeight;
                    multiRankList.get(j).nDocs = (int) Math.ceil(subQueries.get(i).qClassWeight * numHits);
                    multiRankList.get(j).tagClass = subQueries.get(i).qClass;
                    nDoc += multiRankList.get(j).nDocs;
                    j++;
    //                System.out.println("|||||||||||||||||||| Hits: " + multiRankList.get(j).hits.length);
    //                // Normalizing the scores between 0 and 1
    //                float max = multiRankList.get(j).hits[0].score;
    //                float min = multiRankList.get(j).hits[0].score;
    //                for (int k = 1; k < multiRankList.get(j).hits.length; ++k) {
    //                    if(multiRankList.get(j).hits[k].score > max) {
    //                        max = multiRankList.get(j).hits[k].score;
    //                    }
    //                    if(multiRankList.get(j).hits[k].score < min) {
    //                        min = multiRankList.get(j).hits[k].score;
    //                    }
    //                }
    //                for (int k = 0; k < multiRankList.get(j).hits.length; ++k) {
    //                    multiRankList.get(j).hits[k].score = (multiRankList.get(j).hits[k].score - min) / (max - min);
    //                }                    
                }
                

            }
        }
        Collections.sort(multiRankList, new cmpMultiRankList());
        
        //multiRankList.get(0).nDocs = numHits;   // Set max #docs to consider to cover the shortage from other ranklists
        multiRankList.get(0).nDocs = multiRankList.get(0).hits.length;   // Set max #docs to consider to cover the shortage from other ranklists
//        if(nDoc < numHits) {
//            multiRankList.get(0).nDocs += (numHits - nDoc);
//        }


        //return mergeRanklists(multiRankList); // OK
        return mergeRanklists(updateAvgScoreRanklists(multiRankList)); // OK with avg score
        
        //return multiRankList.get(0).hits;
    }
    
    public ScoreDoc[] retrieveCustomized(TRECQuery query) throws Exception { // First retrieved all docs with "SHOULD" clause from whole corpus then filter with city
        
        List<ScoreDoc> hits = new ArrayList<>();
        //List<ScoreDoc> hits = null;
        ScoreDoc[] hitsTemp = null;
        TopDocs topDocs = null;

        TopScoreDocCollector collector = TopScoreDocCollector.create(indexReader.numDocs());
        Query luceneQuery = trecQueryparser.getAnalyzedQuery(query, 1);
//        System.out.println("\n-----------------------------------------------------------------------------");

        //System.out.println("||||||||||||||||||||||||||||||||||||||||||\nluceneQuery: " + luceneQuery + "\n-----------------------------------------------------\nbooleanQuery: " + booleanQuery.toString() + "\n-----------------------------------------------------\ncityQuery: " + cityQuery.toString() + "\n-----------------------------------------------------\ncandidateQuery: " + candidateQuery.toString() + "\n||||||||||||||||||||||||||||||||||||||||||\n");


        System.out.println(query.qid+ ": " +luceneQuery.toString(fieldToSearch));

        indexSearcher.search(luceneQuery, collector); // Formal query
        //indexSearcher.search(booleanQuery, collector); // Formal query AND City matching
        topDocs = collector.topDocs();
        hitsTemp = topDocs.scoreDocs;
        
        int counter = 0;
        if(hitsTemp != null) {
            for (int i = 0; i < hitsTemp.length && counter < numHits+100; ++i) {
                if(query.qcity.equals(indexSearcher.doc(hitsTemp[i].doc).get("cityId"))) {
                    ScoreDoc tempScoreDoc = hitsTemp[i];
                    hits.add(tempScoreDoc);
                    counter++;
                }
            }
        }
        //System.out.println("||||||||| MAX: " + max + "\tMIN: " + min + "\tTOTAL: " + hits.length);


//        // Normalizing the scores between 0 and 1
//        if(counter > 0) {
//            float max = hits.get(0).score;
//            float min = hits.get(hits.size() - 1).score;
//            for (int k = 0; k < hits.size(); ++k) {
//                if(max - min == 0.0f)
//                    hits.get(k).score = 0.5f; // 0.5f;  // Problem: all docs with same scores
//                else
//                    hits.get(k).score = ((hits.get(k).score - min) / (max - min));
//            }            
//        }

        //System.out.println("\n|||||||||||||||||||||||||||||||||||||\n#HITS: " + hits.length + "\n|||||||||||||||||||||||||||||||||||||");


//        // Ani...
//        for (int i = 0; i < hits.length; ++i) {
//            if(hits[i].score <= 0.0) {
//                hits[i].score = 0.5f;
//            }
//        }
//        // Updating scores
//        reRankUsingKDE(hits, query);
//
//        // Sorting hits
//        Arrays.sort(hits, new cmpScoreDoc());

        
        
//        for (int i = 0; i < hits.length; ++i) {
//            System.out.println("\nHITS: " + hits[i].doc + "\t" + hits[i].score);
//            System.out.println("TopDocs: " + topDocs.scoreDocs[i].doc + "\t" + topDocs.scoreDocs[i].score + "\n");
//        }

        
//        SearchResult []results = new SearchResult[hits.length];
//        
//        for (int i = 0; i < hits.length; ++i) {
//            results[i] = new SearchResult(hits[i], indexSearcher.doc(hits[i].doc));
//        }
        
//        Arrays.sort(results, new ReRankAni());


//        for (int i = 0; i < hits.length; ++i) {
//            hits[i] = results[i].docScore;
//        }
//

//        List<NewScore> finalList = new ArrayList<>();
//        Collections.sort(finalList, new Comparator<NewScore>(){
//            @Override
//            public int compare(NewScore t, NewScore t1) {
//                return t.score>t1.score?1:t.score==t1.score?0:-1;
//            }
//        });
//        Collections.sort(hits, new Comparator<ScoreDoc>(){
//            @Override
//            public int compare(ScoreDoc t, ScoreDoc t1) {
//                return t.score>t1.score?1:t.score==t1.score?0:-1;
//            }
//        });
        //.............
        
        if(counter == 0) {
            //System.out.println("Nothing found");
            return null;
        }
        else
            return hits.toArray(new ScoreDoc[0]);
    }

    public ScoreDoc[] retrieve(TRECQuery query) throws Exception {

        ScoreDoc[] hits = null;
        TopDocs topDocs = null;

        TopScoreDocCollector collector = TopScoreDocCollector.create(numHits);
        Query luceneQuery = trecQueryparser.getAnalyzedQuery(query, 1);
        Query cityQuery = new TermQuery(new Term("cityId", query.qcity));
        Query candidateQuery = new TermQuery(new Term("qQID", query.qid));
        //Query candidateQuery = trecQueryparser.getAnalyzedQuery(query, 3); // for parsing QID only

        
//        System.out.println("Ani Query: " + query.qtitle);
        //System.out.println("Ani Query: " + query.luceneQuery);

//        System.out.println("Ani sub-queries:");
//        for (int i = 0; i < TagsClusters.size(); ++i) {
//            if(!"-1".equals(subQueries.get(i).qClass))
//                //System.out.println(i + ": " + subQueries.get(i).qClass + " (" + subQueries.get(i).qClassWeight + ")\t" + subQueries.get(i).qtitle);
//                System.out.print("(" + subQueries.get(i).qtitle + ") ");
//        }
//        System.out.println("\n-----------------------------------------------------------------------------");
        
        PhraseQuery candidateQueryPhrase = new PhraseQuery();
        candidateQueryPhrase.add(new Term("qQID", query.qid));


        BooleanQuery booleanQuery = new BooleanQuery();
        booleanQuery.add(luceneQuery, BooleanClause.Occur.SHOULD);
        booleanQuery.add(cityQuery, BooleanClause.Occur.MUST); // City matching is MUST
        //booleanQuery.add(candidateQuery, BooleanClause.Occur.MUST);
        
        //booleanQuery.add(candidateQueryPhrase, BooleanClause.Occur.MUST);


        //System.out.println("||||||||||||||||||||||||||||||||||||||||||\nluceneQuery: " + luceneQuery + "\n-----------------------------------------------------\nbooleanQuery: " + booleanQuery.toString() + "\n-----------------------------------------------------\ncityQuery: " + cityQuery.toString() + "\n-----------------------------------------------------\ncandidateQuery: " + candidateQuery.toString() + "\n||||||||||||||||||||||||||||||||||||||||||\n");


        System.out.println(query.qid+ ": " +luceneQuery.toString(fieldToSearch));

        //indexSearcher.search(luceneQuery, collector); // Formal query
        indexSearcher.search(booleanQuery, collector); // Formal query AND City matching
        topDocs = collector.topDocs();
        hits = topDocs.scoreDocs;
        
//        // Normalizing the scores between 0 and 1
//        float max = hits[0].score;
//        float min = hits[hits.length - 1].score;
//        for (int k = 0; k < hits.length; ++k) {
//            if(max - min == 0.0f)
//                hits[k].score = 1.0f; // 0.5f;  // Problem: all docs with same scores
//            else
//                hits[k].score = ((hits[k].score - min) / (max - min));
//        }
        //System.out.println("||||||||| MAX: " + max + "\tMIN: " + min + "\tTOTAL: " + hits.length);


        //System.out.println("\n|||||||||||||||||||||||||||||||||||||\n#HITS: " + hits.length + "\n|||||||||||||||||||||||||||||||||||||");


//        // Ani...
//        for (int i = 0; i < hits.length; ++i) {
//            if(hits[i].score <= 0.0) {
//                hits[i].score = 0.5f;
//            }
//        }
//        // Updating scores
//        reRankUsingKDE(hits, query);
//
//        // Sorting hits
//        Arrays.sort(hits, new cmpScoreDoc());

        
        
//        for (int i = 0; i < hits.length; ++i) {
//            System.out.println("\nHITS: " + hits[i].doc + "\t" + hits[i].score);
//            System.out.println("TopDocs: " + topDocs.scoreDocs[i].doc + "\t" + topDocs.scoreDocs[i].score + "\n");
//        }

        
//        SearchResult []results = new SearchResult[hits.length];
//        
//        for (int i = 0; i < hits.length; ++i) {
//            results[i] = new SearchResult(hits[i], indexSearcher.doc(hits[i].doc));
//        }
        
//        Arrays.sort(results, new ReRankAni());


//        for (int i = 0; i < hits.length; ++i) {
//            hits[i] = results[i].docScore;
//        }
//

//        List<NewScore> finalList = new ArrayList<>();
//        Collections.sort(finalList, new Comparator<NewScore>(){
//            @Override
//            public int compare(NewScore t, NewScore t1) {
//                return t.score>t1.score?1:t.score==t1.score?0:-1;
//            }
//        });
//        Collections.sort(hits, new Comparator<ScoreDoc>(){
//            @Override
//            public int compare(ScoreDoc t, ScoreDoc t1) {
//                return t.score>t1.score?1:t.score==t1.score?0:-1;
//            }
//        });
        //.............
        
        if(hits == null)
            System.out.println("Nothing found");

        return hits;
    }
    
    // For each query term it takes topK semantically similar terms from Word2vec model
    public TRECQuery getExpandedQuery(TRECQuery query, int topK) throws Exception {
        String[] termsRaw = trecQueryparser.getAnalyzedQuery(query, 1).toString(fieldToSearch).replace("(", "").replace(")", "").split(" ");

        List<String> terms = getUniqTerms(termsRaw);
        List<String> termsExpanded = new ArrayList<>();

        for (int i = 0; i < terms.size(); ++i) {
            //termsExpanded.add(terms.get(i));
            if(getTermIndex(terms.get(i)) >= 0) {
                List<Word2vec> W2V = topkW2Vmodel(getTermIndex(terms.get(i)));
                for (int j = 0; j < topK && j < W2V.size(); ++j) {
                    termsExpanded.add(W2V.get(j).term);
                }
            }
        }
        List<String> termsExpandedUniq = getUniqTerms(termsExpanded.toArray(new String[0]));
        
        //query.qtitle = "";
        for (int i = 0; i < termsExpandedUniq.size(); ++i) {
            //System.out.print(termsExpandedUniq.get(i) + " ");
            query.qtitle += termsExpandedUniq.get(i) + " ";
        }
//        System.out.println();
//        System.exit(1);
        return query;
    }

    // For each query Q (q1, q2, ..., qn) it takes topM terms based on KDE score from the set of topK semantically similar (Word2vec model) terms for each query term q_i i.e. m of k terms
    public TRECQuery getExpandedQueryW2VKDE(TRECQuery query, int topM, int topK) throws Exception {
        String[] termsRaw = trecQueryparser.getAnalyzedQuery(query, 1).toString(fieldToSearch).replace("(", "").replace(")", "").split(" ");

        List<String> terms = getUniqTerms(termsRaw);
        List<String> termsExpanded = new ArrayList<>();
        List<Word2vec> observedTerms = new ArrayList<>();
        List<Word2vec> candidateTerms = new ArrayList<>();
        
        if(terms.size() <= 1)
            return query;

        for (int i = 0; i < terms.size(); ++i) {
            //termsExpanded.add(terms.get(i));
            if(getTermIndex(terms.get(i)) >= 0) {
                Word2vec temp = new Word2vec();
                temp.term = terms.get(i);
                temp.vector = convertVectorStringToFloat(W2Vmodel.get(getTermIndex(terms.get(i))));
                observedTerms.add(temp);
                
                List<Word2vec> W2V = topkW2Vmodel(getTermIndex(terms.get(i)));
                
                for (int j = 0; j < topK && j < W2V.size(); ++j) {
                    termsExpanded.add(W2V.get(j).term);
                }
            }
        }
        List<String> termsExpandedUniq = getUniqTerms(termsExpanded.toArray(new String[0]));
        
        float[] weightArray = new float[observedTerms.size()]; // weight array for KDE. Weights for observed terms
        for (int i = 0; i < observedTerms.size(); ++i) {
            //weightArray[i] = 1.0f;  // 1.0 means equal weights. Try using tf, Cf, collection probability, tf*IDF etc.
            //weightArray[i] = (float) (getCF(observedTerms.get(i).term) * getIdf(observedTerms.get(i).term));
            //weightArray[i] = (float) (getCF(observedTerms.get(i).term));
            weightArray[i] = (float) (getCFNormalized(observedTerms.get(i).term));
        }
        for (int i = 0; i < termsExpandedUniq.size(); ++i) {
            Word2vec temp = new Word2vec();
            temp.term = termsExpandedUniq.get(i);
            temp.vector = convertVectorStringToFloat(W2Vmodel.get(getTermIndex(termsExpandedUniq.get(i))));
            temp.KDEScore = KDEScoreForTermSelect(temp.vector, observedTerms, weightArray, observedTerms.size(), 1, 1.0f);  // using sigma=1.0, h=1
            candidateTerms.add(temp);
        }
        Collections.sort(candidateTerms, new cmpW2VKDESim());
        
        //query.qtitle = "";
        for (int i = 0; i < topM && i < candidateTerms.size(); ++i) {
            //System.out.print(termsExpandedUniq.get(i) + " ");
            query.qtitle += candidateTerms.get(i).term + " ";
        }
//        System.out.println();
//        System.exit(1);
        return query;
    }
    
    public float[] convertVectorStringToFloat(String[] vector) throws Exception {
        float[] vectorFloat = new float[vector.length];
        for (int i = 1; i < vector.length; ++i) {
            vectorFloat[i-1] = Float.parseFloat(vector[i]);
        }
        return vectorFloat;
    }
    
    public List<String> getUniqTerms(String[] termsRaw) throws Exception {
        List<String> terms = new ArrayList<>();
        
        Arrays.sort(termsRaw);
        
        String term = termsRaw[0];
        for (int i = 1; i < termsRaw.length; ++i) {
            if(!term.equals(termsRaw[i])) {
                terms.add(term);
                term = termsRaw[i];
            }
        }
        terms.add(term);

        return terms;
    }
    
    // Returns estimated KDE score of term x (vector representation of x) based on x_i terms where i=0, 1, ..., n-1
    // f_w(x) = 1/nh ∑w_i . K((x - x_i) / h) for i=1, 2, ..., n [weighted KDE with gaussian kernel function K(.), bandwidth h]
    public float KDEScoreForTermSelect(float[] x, List<Word2vec> xArray, float[] wArray, int n, int h, float sigma) throws Exception {
        float score = 0.0f, expPart;

        for (int i = 0; i < n; ++i) {
            expPart = (float) Math.exp(-( Math.pow(cosineSimilarity(x, xArray.get(i).vector), 2) / 2 * Math.pow(sigma, 2) * Math.pow(h, 2)));
            score += wArray[i] / (Math.sqrt(2 * Math.PI) * sigma) * expPart;
        }
        score /= n * h;
        
        return score;
    }

    
    public void reRankUsingKDE(ScoreDoc[] hits, TRECQuery query) throws Exception {
        Document[] docList = new Document[hits.length];
        double[] wArray = new double[hits.length]; // weight array
        double[] xArray = new double[hits.length]; // latitude array
        double[] yArray = new double[hits.length]; // longitude array
        
        
        for (int i = 0; i < hits.length; ++i) {
            docList[i] = indexSearcher.doc(hits[i].doc);
//            if(hits[i].score <= 0.0) {
//                hits[i].score = 0.5f;
//            }
//            
            // Get access to the location of each document
            //docList[i].get("lat"); /* or (same) */ // indexSearcher.doc(hits[i].doc).get("lat");
            //docList[i].get("lng"); /* or (same) */ // indexSearcher.doc(hits[i].doc).get("lng");

            //update the scores...
        //System.out.println("\nLat: " + docList[i].get("lat") + "\tLng:" + docList[i].get("lng") + "\n");
        }
        
//        for (int i = 0; i < hits.length; ++i) {
//            wArray[i]=hits[i].score;
//            xArray[i]=Double.parseDouble(docList[i].get("lat"));
//            yArray[i]=Double.parseDouble(docList[i].get("lng"));
//        }
        
        // n = 1 when estimating all docs based on the query (as each query has a single location point)
        wArray[0] = 1.0;
        xArray[0] = Double.parseDouble(query.qlat);
        yArray[0] = Double.parseDouble(query.qlng);

        for (int i = 0; i < hits.length; ++i) {
            hits[i].score = (float) KDEScore(Double.parseDouble(docList[i].get("lat")), Double.parseDouble(docList[i].get("lng")), xArray, yArray, wArray, 1, 1);
            System.out.println("Score: "+hits[i].score);
        }
    }
    
    // Returns estimated KDE score of (x, y) point based on (x_i, y_i) points where i=0, 1, ..., n-1
    // f_w(x) = 1/nh ∑w_i . K((x - x_i) / h) for i=1, 2, ..., n [weighted KDE with gaussian kernel function K(.), bandwidth h]
    public static double KDEScore(double x, double y, double[] xArray, double[] yArray, double[] wArray, int n, int h) {
        double latScore = 0.0, lngScore = 0.0, sigma, expPart;
        
        //expPart = Math.exp(-(Math.pow(distance(Double.parseDouble(x.get("lat")), Double.parseDouble(query.qlat), Double.parseDouble(x.get("lng")), Double.parseDouble(query.qlng), 0.0, 0.0), 2) / 2 * ));
        
        // Estimating latitude x
        sigma = variance(xArray, n);
        for (int i = 0; i < n; ++i) {
            expPart = Math.exp(-( Math.pow((x - xArray[i]), 2) / 2 * Math.pow(sigma, 2) * Math.pow(h, 2)));
            latScore += wArray[i] / (Math.sqrt(2 * Math.PI) * sigma) * expPart;
        }
        latScore /= (double)n * h;

        // Estimating longitude y
        sigma = variance(yArray, n);
        for (int i = 0; i < n; ++i) {
            expPart = Math.exp(-( Math.pow((y - yArray[i]), 2) / 2 * Math.pow(sigma, 2) * Math.pow(h, 2)));
            lngScore += wArray[i] / (Math.sqrt(2 * Math.PI) * sigma) * expPart;
        }
        lngScore /= (double)n * h;

        
        return latScore * lngScore;
    }
    
    static double variance(double a[], int n)
    {   double sum = 0.0, sqDiff = 0.0, var;
    
        if(n == 1) // You don't want to do this. Estimating a candidate data point based on a single pivot data point? (Think to generate random 3-4 coordinates)
            return 0.00001;
    
        for (int i = 0; i < n; i++)
            sum += a[i];
        double mean = (double)sum / (double)n;
        for (int i = 0; i < n; i++) 
            sqDiff += (a[i] - mean) * (a[i] - mean);
        
        //System.out.println("\n------------------------------------------\nn = " + n + "\tMean: " + mean + "\tsqDiff: " + sqDiff + "\tVar: " + (double)sqDiff / (n-1) + "\n------------------------------------------\n");
        var = (double)sqDiff / (n-1);
        
        if(var == 0.0)
            return 0.00001; // Returns a very small number, to keep the nature of the KDE equation intact.
        else
            return var;
    }    
    
/* Calculate distance between two points in latitude and longitude taking
 * into account height difference. If you are not interested in height
 * difference pass 0.0. Uses Haversine method as its base.
 * 
 * lat1, lon1 Start point lat2, lon2 End point el1 Start altitude in meters
 * el2 End altitude in meters
 * @returns Distance in Meters
 */
    public static double distance(double lat1, double lat2, double lon1,
double lon2, double el1, double el2) {

        final int R = 6371; // Radius of the earth

        double latDistance = Math.toRadians(lat2 - lat1);
        double lonDistance = Math.toRadians(lon2 - lon1);
        double a = Math.sin(latDistance / 2) * Math.sin(latDistance / 2)
                + Math.cos(Math.toRadians(lat1)) * Math.cos(Math.toRadians(lat2))
                * Math.sin(lonDistance / 2) * Math.sin(lonDistance / 2);
        double c = 2 * Math.atan2(Math.sqrt(a), Math.sqrt(1 - a));
        double distance = R * c * 1000; // convert to meters

        double height = el1 - el2;

        distance = Math.pow(distance, 2) + Math.pow(height, 2);

        return Math.sqrt(distance);
    }
    public void loadTagsClustersWeight (String path) throws Exception {
        File file = new File(path);
        FileReader fr = new FileReader(file);
        BufferedReader br = new BufferedReader(fr);
        String line;
        TagsClustersWeight = new ArrayList<>();
        while ((line = br.readLine()) != null) {
            TagsClustersWeight.add(line.split(" "));
            //Arrays.sort(TagsClustersWeight.get(i));
        }
        br.close();
        Collections.sort(TagsClustersWeight, new cmpW2VModel());
    }

    
    public void loadTagsClusters (String path) throws Exception {
        File file = new File(path);
        FileReader fr = new FileReader(file);
        BufferedReader br = new BufferedReader(fr);
        String line;
        TagsClusters = new ArrayList<String []>();
        int i = 0;
        while ((line = br.readLine()) != null) {
            TagsClusters.add(i, line.split(" "));
            Arrays.sort(TagsClusters.get(i));
            i++;
        }
        br.close();
//        for (i = 0; i < TagsClusters.size(); ++i) {
//            for (int j = 0; j < TagsClusters.get(i).length; ++j) {
//                System.out.print(TagsClusters.get(i)[j] + " ");
//            }
//            System.out.println("");
//        }
        //System.out.println(Arrays.binarySearch(TagsClusters.get(0), "archeology"));
        //System.exit(1);

    }
    
    public void loadW2Vmodel (String path) throws Exception { // Load Word2vec model

        File file = new File(path);
        FileReader fr = new FileReader(file);
        BufferedReader br = new BufferedReader(fr);
        String line;
        String[] keyTermVector = new String[201];
        //List<String []> W2Vmodel = new ArrayList<String []>();
        W2Vmodel = new ArrayList<String []>();
        int i = 0;
        while ((line = br.readLine()) != null) {
            W2Vmodel.add(i, line.split(" "));
            i++;
        }
        br.close();
        Collections.sort(W2Vmodel, new cmpW2VModel());
//        keyTermVector[0] = "beer";
//        System.out.println(Collections.binarySearch(W2Vmodel, keyTermVector, new cmpW2VModel()));
//
//        System.out.println(W2Vmodel.size() + "\t" + W2Vmodel.get(0).length);
//        for (i = 0; i < W2Vmodel.size(); ++i) {
//            for (int j = 0; j < W2Vmodel.get(i).length; ++j) {
//                System.out.print(W2Vmodel.get(i)[j] + " ");
//            }
//            System.out.println("");
//        }
//          System.exit(1);

        
//        int nDoc = indexReader.numDocs();
//        int nTopic = queries.size();
//        System.out.println("nTopic: " + nTopic);
//        for (int i = 0; i < nTopic; ++i) {
//            System.out.println("Query " + i + ": " + trecQueryparser.getAnalyzedQuery(queries.get(i), 1));
//        }
//        String content = indexSearcher.doc(0).getField("full-content").stringValue();
//        String[] terms = content.split(" ");
//        System.out.println("Content: " + terms.length);
//        String content1 = trecQueryparser.getAnalyzedQuery(queries.get(0), 1).toString();
//        String[] terms1 = content1.split(" ");
//        System.out.println("Content: " + terms1[1].replace("full-content:", ""));

        //System.out.println("Query: " + queries.get(0).qtitle);
//        System.out.println("Query: " + trecQueryparser.getAnalyzedQuery(queries.get(0), 1));
//        System.out.println("nDoc: " + nDoc);
//        System.out.println("ID: " + indexSearcher.doc(0).getField("docid").stringValue());
//        System.out.println("Content: " + indexSearcher.doc(0).getField("full-content").stringValue());
//        System.exit(1);
//        File file = new File("/store/TCD/TREC_CS/Wor2vec/trunk/64K_BOW_analysed");
//        file.createNewFile();
//        FileWriter writer = new FileWriter(file, true);
//        
//        for (int i = 0; i < nDoc; ++i) {
//            //Document doc = indexReader.document(i);
//            Document doc = indexSearcher.doc(i);
//            //System.out.println("Doc " + i + ": " + doc.getField("qQID").stringValue());
//            System.out.println("Doc: " + i + " Done!");
//            writer.write(doc.getField("full-content").stringValue() + " ");
//            writer.flush();
//        }
//        writer.close();
//        System.exit(1);
    }
    
    public int getTermIndex (String term) throws Exception {    // Get index of term in W2V model
        String[] keyTermVector = new String[201];
        keyTermVector[0] = term;
        return Collections.binarySearch(W2Vmodel, keyTermVector, new cmpW2VModel());
    }
    
    public List<Word2vec> topkW2Vmodel (int termIndex) throws Exception { // Returns list of terms (class Word2vec) sorted on cosine similarity scores
        List<Word2vec> W2V = new ArrayList<>();

        if(termIndex >= 0) {
            float[] vec1 = new float[200];
            for (int j = 1; j < W2Vmodel.get(termIndex).length; ++j) {
                vec1[j-1] = Float.parseFloat(W2Vmodel.get(termIndex)[j]);
            }
            for (int i = 0; i < W2Vmodel.size(); ++i) {
                if(i != termIndex) {
                    float[] vec2 = new float[200];
                    for (int j = 1; j < W2Vmodel.get(i).length; ++j) {
                        vec2[j-1] = Float.parseFloat(W2Vmodel.get(i)[j]);
                    }
                    Word2vec tempW2v = new Word2vec();
                    tempW2v.term = W2Vmodel.get(i)[0];
                    tempW2v.vector = vec2;
                    tempW2v.consineScore = cosineSimilarity(vec1, vec2);
                    W2V.add(tempW2v);
                }
            }
            Collections.sort(W2V, new cmpW2VCosineSim());
            return W2V;
        }
        
        return W2V;
    }
    
    public float cosineSimilarity (float[] a, float[] b) throws Exception { // Returns cosine similarity between two vectors
        float sum = 0.0f, sum1 = 0.0f, sum2 = 0.0f;

        for (int i = 0; i < a.length; ++i) {
            sum += a[i] * b[i];
            sum1 += Math.pow(a[i], 2);
            sum2 += Math.pow(b[i], 2);
        }
        sum /= (Math.sqrt(sum1) * Math.sqrt(sum2));
        return sum;
    }

    
    public List<TRECQuery> generateSubQueries(TRECQuery query) throws Exception {
        
        List<TRECQuery> subQueries = new ArrayList<>();
        String[] queryNo = new String[TagsClustersWeight.get(0).length];
        queryNo[0] = query.qid;

        String[] terms = query.qtitle.split(" ");
        
        for (int i = 0; i < TagsClusters.size(); ++i) {
            subQueries.add(new TRECQuery());
            //subQueries.add(i, query); // .add korle, ith index-er val change korle sobgulo change hoye jacche :-o ; TRECQuery query copy korte chaichilam as all elements in List<TRECQuery> subQueries
            subQueries.get(i).fieldToSearch = query.fieldToSearch;
            subQueries.get(i).qid = query.qid;
            subQueries.get(i).qcity = query.qcity;
            subQueries.get(i).qlat = query.qlat;
            subQueries.get(i).qlng = query.qlng;
            subQueries.get(i).luceneQuery = query.luceneQuery;
            subQueries.get(i).qClass = "-1";
            subQueries.get(i).qClassWeight = 0.0f;
            subQueries.get(i).qtitle = "";
        }

        for (String term : terms) {
            for (int i = 0; i < TagsClusters.size(); ++i) {
                if(Arrays.binarySearch(TagsClusters.get(i), term) >= 0) {
                    subQueries.get(i).qtitle += term + " ";
                    subQueries.get(i).qClass = Integer.toString(i);
                }
            }
        }
        float sum = 0.0f;
        for (int i = 0; i < subQueries.size(); ++i) {
            if(!"-1".equals(subQueries.get(i).qClass)) {
                if(Collections.binarySearch(TagsClustersWeight, queryNo, new cmpW2VModel()) >= 0) {
                    subQueries.get(i).qClassWeight = Float.parseFloat(TagsClustersWeight.get(Collections.binarySearch(TagsClustersWeight, queryNo, new cmpW2VModel()))[i+1]);
                    sum += subQueries.get(i).qClassWeight;
                    //System.out.println(i + "-th sub-query weight\t" + Collections.binarySearch(TagsClustersWeight, queryNo, new cmpW2VModel()) + "-th query\t" + (i+1) + "-th entry");
                }
            }
        }

        // Weight adjustment
        for (int i = 0; i < subQueries.size(); ++i) {
            if(!"-1".equals(subQueries.get(i).qClass) && sum != 0.0f) {
                subQueries.get(i).qClassWeight += ((1.0f - sum) * (subQueries.get(i).qClassWeight / sum));  // Adjusting the %age shortage for phase 1 missing tags
            }
        }
//        System.out.println("sum: " + sum);
        sum = 0.0f;
        //System.out.println("----------------------------------------------------------");
        for (int i = 0; i < TagsClusters.size(); ++i) {
            if(!"-1".equals(subQueries.get(i).qClass)) {
                //System.out.println(i + ": " + subQueries.get(i).qClass + " (" + subQueries.get(i).qClassWeight + ")\t" + subQueries.get(i).qtitle);
                sum += subQueries.get(i).qClassWeight; 
            }
        }
        //System.out.println("sum: " + sum);
//        System.exit(1);
        return subQueries;
    }

    // Returns IDF of 'term'
    public double getIdf(String term) throws IOException {
        int docCount = indexReader.maxDoc();      // total number of documents in the index
        Term termInstance = new Term(fieldToSearch, term);
        long df = indexReader.docFreq(termInstance);       // DF: Returns the number of documents containing the term

        double idf;
        idf = Math.log((float)(docCount)/(float)(df+1));

        return idf;
    }

    // Returns TF of 'term' in document 'docID'
    public long getTF(String term, int docID) throws Exception {
        DocumentVector dv = new DocumentVector();
        dv.field = fieldToSearch;
        return dv.getTf(term, dv.getDocumentVector(docID, indexReader));
    }

    // Returns collection TF i.e. CF of 'term'
    public long getCF(String term) throws Exception {
        Term termInstance = new Term(fieldToSearch, term);
        return indexReader.totalTermFreq(termInstance); // CF: Returns the total number of occurrences of term across all documents (the sum of the freq() for each doc that has this term).
    }

    // Returns normalized collection TF (CF) of 'term' i.e. the collection probability of 'term'
    public float getCFNormalized(String term) throws Exception {
        DocumentVector dv = new DocumentVector();
        Term termInstance = new Term(fieldToSearch, term);
        long termFreq = indexReader.totalTermFreq(termInstance); // CF: Returns the total number of occurrences of term across all documents (the sum of the freq() for each doc that has this term).

        return (float) termFreq / (float) dv.getVocabularySize(indexReader, fieldToSearch);
    }

    public void retrieveAll() throws Exception {
        
        loadTagsClusters(TRECCSTagsClustersFilePath);
        loadTagsClustersWeight(TRECCSTagsClustersWeightFilePath);
        loadW2Vmodel(W2VFilePath); // W2VFilePath // Sample: /store/TCD/TREC_CS/Wor2vec/trunk/a
        
//        String[] terms = {"Alone", "Family", "Friends", "Business", "Holiday", "Autumn", "Spring", "Summer"};
//        List<Word2vec> W2V = new ArrayList<>();
//        int termIndex = getTermIndex("beer1");
//        W2V = topkW2Vmodel(termIndex);
//        for (int i = 0; i < 50; ++i) {
//            System.out.println(W2V.get(i).term + " " + W2V.get(i).consineScore);
//        }
//
//        System.out.println(termIndex);
        
//        System.out.println("CF: " + getCF("holidai"));
//        System.out.println("TF: " + getTF("holiday", 7837));
//        System.out.println("IDF: " + getIdf("holidai"));
//        System.exit(1);

        ScoreDoc[] hits = null;

        for (TRECQuery query : queries) {

            //hits = retrieve(query);
            hits = retrieveCustomized(query);
            //hits = retrieveMultiQuery(query);
            //hits = retrieveMultiQuery1(query);
            int hits_length = hits.length;
            System.out.println(query.qid + ": documents retrieve: " +hits_length);
            StringBuffer resBuffer = new StringBuffer();

            for (int i = 0; i < hits_length; ++i) {
                int luceneDocId = hits[i].doc;
                Document d = indexSearcher.doc(luceneDocId);
                resBuffer.append(query.qid).append("\tQ0\t").
                    append(d.get(FIELD_ID)).append("\t").
                    append((i)).append("\t").
                    append(hits[i].score).append("\t").
                    append(runName).append("\n");
                    //append(runName).append("\t").append(d.get("lat")).append("\n");
                    //append(runName).append("\t").append(query.qlat).append(", ").append(query.qlng).append("\n");
            }
            resFileWriter.write(resBuffer.toString());
        }
        resFileWriter.close();
        System.out.println("The result is saved in: "+resPath);

    }

    public static void main(String[] args) throws IOException, Exception {

        WebDocSearcher_TRECCS collSearcher = null;

        String usage = "java Wt10gSearcher <properties-file>\n"
            + "Properties file must contain:\n"
            + "1. indexPath: Path of the index\n"
            + "2. fieldToSearch: Name of the field to use for searching\n"
            + "3. queryPath: Path of the query file (in proper xml format)\n"
            + "4. queryFieldFlag: 1-title, 2-title+desc, 3-title+desc+narr\n"
            + "5. similarityFunction: 0.DefaultSimilarity, 1.BM25Similarity, 2.LMJelinekMercerSimilarity, 3.LMDirichletSimilarity\n"
            + "6. param1: \n"
            + "7. [param2]: optional if using BM25";

        /* // uncomment this if wants to run from inside Netbeans IDE
        args = new String[1];
        args[0] = "searcher.properties";
        //*/

        if(0 == args.length) {
            System.out.println(usage);
            System.exit(1);
        }

        System.out.println("Using properties file: "+args[0]);
        collSearcher = new WebDocSearcher_TRECCS(args[0]);

        collSearcher.retrieveAll();
    }

}
